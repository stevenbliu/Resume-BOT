version: '3.8'

services:
  llm:
    build: ./llm
    container_name: tinyllama-vllm
    ports:
      - "11434:11434"
    runtime: nvidia
    environment:
      - HF_HUB_ENABLE_HF_TRANSFER=1
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    restart: unless-stopped

  rag:
    build: ./rag
    container_name: rag-service
    ports:
      - "8000:8000"
    depends_on:
      - llm
    environment:
      - RAG_SERVICE_URL=http://llm:11434
    restart: unless-stopped

  backend:
    build: ./backend
    container_name: resume-backend
    ports:
      - "8001:8001"
    depends_on:
      - llm
    environment:
      - OLLAMA_BASE_URL=http://llm:11434
    restart: unless-stopped

  frontend:
    build:
      context: ./frontend
    container_name: resume-frontend
    ports:
      - "80:80"
    restart: unless-stopped

# Optional volumes - remove if not used
# volumes:
#   ollama_data:
